{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/raroog/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 3\n",
    "\n",
    "$\\frac{d^2}{dx^2}\\Psi+\\frac{1}{5}\\frac{d}{dx}\\Psi+\\Psi=-\\frac{1}{5}\\exp(-\\frac{x}{5})\\cos(x)$\n",
    "\n",
    "With boundary initial condition $\\Psi(0)=0$, $\\frac{d}{dx}\\Psi(0)=1$ and domain $x\\in[0,2]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = np.arange(0, 2., 0.2) + 1e-8\n",
    "X_train = X_train.reshape(-1,1)\n",
    "X_test = np.arange(0, 2., 0.01) + 1e-8\n",
    "X_test = X_test.reshape(-1,1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inits = [{'variable':0, 'value':0, 'type':'dirichlet',\n",
    "        'function':lambda X: tf.constant(0., dtype='float64', shape=(X.shape[0],1))},\n",
    "        {'variable':0, 'value':0, 'type':'neumann',\n",
    "        'function':lambda X: tf.constant(1., dtype='float64', shape=(X.shape[0],1))}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TrialSolution(tf.keras.models.Model):\n",
    "  def __init__(self, conditions, n_i, n_h, n_o=1, activation='sigmoid', equation_type='ODE'):\n",
    "    super(TrialSolution, self).__init__()\n",
    "    \n",
    "    # Dimensions of the network\n",
    "    self.n_i = n_i\n",
    "    self.n_h = n_h\n",
    "    self.n_o = n_o\n",
    "    \n",
    "    # Boundary conditions\n",
    "    self.conditions = conditions\n",
    "    \n",
    "    # Shallow network\n",
    "    self.hidden_layer = tf.keras.layers.Dense(units=self.n_h, activation=activation)\n",
    "    self.output_layer = tf.keras.layers.Dense(units=self.n_o, activation='linear')\n",
    "    \n",
    "  def call(self, X):\n",
    "    X = tf.convert_to_tensor(X)\n",
    "    response = self.hidden_layer(X)\n",
    "    response = self.output_layer(response)\n",
    "    \n",
    "    # Automatic conditions incorporation including Neumann BCs\n",
    "    # It should be used to generate the *call* method instead of calculating it every damned time\n",
    "        \n",
    "    boundary_value = tf.constant(0., dtype='float64', shape=response.get_shape())\n",
    "    \n",
    "    for condition in self.conditions:\n",
    "      vanishing = tf.constant(1., dtype='float64', shape=response.get_shape())\n",
    "      temp_bc = 0\n",
    "      if condition['type'] == 'dirichlet':\n",
    "        temp_bc = tf.reshape(condition['function'](X), shape=boundary_value.shape)           \n",
    "        for vanisher in self.conditions:\n",
    "          if vanisher['variable'] != condition['variable'] and vanisher['value'] != condition['value']:\n",
    "            if vanisher['type'] == 'dirichlet':\n",
    "              vanishing *= (X[:, vanisher['variable']]\n",
    "                                        - tf.constant(vanisher['value'], dtype='float64', shape=boundary_value.shape))\n",
    "            elif vanisher['type'] == 'neumann':\n",
    "              vanishing *= (X[:, vanisher['variable']]\n",
    "                                        - tf.constant(vanisher['value'], dtype='float64', shape=boundary_value.shape))\n",
    "        boundary_value += temp_bc * vanishing\n",
    "        response *= (tf.constant(condition['value'], dtype='float64', shape=boundary_value.shape)\n",
    "                     - tf.reshape(X[:, condition['variable']], shape=boundary_value.shape))\n",
    "      elif condition['type'] == 'neumann':\n",
    "        temp_bc = (tf.reshape(condition['function'](X), shape=boundary_value.shape)\n",
    "                   * tf.reshape(X[:, condition['variable']], shape=boundary_value.shape))\n",
    "        boundary_value = temp_bc\n",
    "        response *= (tf.constant(condition['value'], dtype='float64', shape=boundary_value.shape)\n",
    "                     - tf.reshape(X[:, condition['variable']], shape=boundary_value.shape))  \n",
    "    response += boundary_value\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The trial solution for this case is $\\Psi(x)=x + x^2N(x)$.\n",
    "The first function below is the function $A(x)=x$\n",
    "and the second function is the function $B(x)=x^2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ts = TrialSolution(conditions=inits, n_i=1, n_h=10, n_o=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the loss function for a single point and a whole set\n",
    "\n",
    "The loss function is based on the formula:\n",
    "$$Loss(N)=\\sum_i \\left(L\\Psi(x_i, N(x_i))-f(x_i,\\Psi(x_i, N(x_i))) \\right)^2$$\n",
    "Where $N(x)$ is the neural network and $L$ is some differential operator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def diff_loss(network, inputs):\n",
    "  # Compute the gradients\n",
    "  with tf.GradientTape() as tape2:\n",
    "    with tf.GradientTape() as tape:\n",
    "      inputs = tf.convert_to_tensor(inputs)\n",
    "      tape.watch(inputs)\n",
    "      tape2.watch(inputs)\n",
    "      response = network(inputs)  \n",
    "    grads = tape.gradient(response, inputs)\n",
    "  laplace = tape2.gradient(grads, inputs)\n",
    "  \n",
    "  # Compute the loss\n",
    "  loss = tf.square(laplace + tf.constant(0.2, dtype='float64')*grads + response\n",
    "          + tf.constant(0.2, dtype='float64')*tf.exp( tf.constant(-0.2, dtype='float64') * inputs)\n",
    "                   * tf.cos(inputs))\n",
    "  return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.01)\n",
    "train_loss = tf.keras.metrics.Mean('train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(X):\n",
    "  # Online learning\n",
    "  for i in X:\n",
    "    with tf.GradientTape() as tape:\n",
    "      loss = diff_loss(ts, tf.reshape(i, shape=(1,1)))\n",
    "    gradients = tape.gradient(loss, ts.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, ts.trainable_variables))\n",
    "  \n",
    "  train_loss(diff_loss(ts, X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check if everything is fine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=119, shape=(10, 1), dtype=float64, numpy=\n",
       "array([[1.00000000e-08],\n",
       "       [2.04050755e-01],\n",
       "       [4.15765075e-01],\n",
       "       [6.34527507e-01],\n",
       "       [8.59812173e-01],\n",
       "       [1.09122143e+00],\n",
       "       [1.32851771e+00],\n",
       "       [1.57164710e+00],\n",
       "       [1.82075389e+00],\n",
       "       [2.07618592e+00]])>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ts(tf.convert_to_tensor(X_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0030667903\n",
      "0.0015638769\n",
      "0.0010593429\n",
      "0.00080488593\n",
      "0.0006507894\n",
      "0.0005471063\n",
      "0.0004723987\n",
      "0.00041592174\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 100000\n",
    "for epoch in range(EPOCHS):\n",
    "  train_step(X_train)\n",
    "  if (epoch+1) % 1000 == 0:\n",
    "    print(train_loss.result().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the results \n",
    "\n",
    "The numerical solution (training set - red, valdiaiton set - green) along with the analytical solution (blue)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_train = ts.call(tf.convert_to_tensor(X_train, dtype='float64')).numpy()\n",
    "pred_test = ts(tf.convert_to_tensor(X_test, dtype='float64')).numpy()\n",
    "plt.scatter(X_train, pred_train, c='r', label='Numerical - Training', marker='+', s=30)\n",
    "plt.plot(X_test, pred_test, c='g', label='Numerical - Test')\n",
    "plt.plot(X_test, np.exp(-0.2*X_test)*np.sin(X_test), c='b', label='Analytic')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the errors on the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(X_train, pred_train - np.exp(-0.2*X_train)*np.sin(X_train), label='Error - Train')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the errors on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(X_test, pred_test - np.exp(-0.2*X_test)*np.sin(X_test), label='Error - Test')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mean loss calculated on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "diff_loss(ts, X_test).numpy().mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mean absolute error on the train set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.abs(pred_train - np.exp(-0.2*X_train)*np.sin(X_train)).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mean absolute error on the test set - interpolation error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.abs(pred_test - np.exp(-0.2*X_test)*np.sin(X_test)).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
