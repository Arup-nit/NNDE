{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import precision_MC as pmc\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W1104 22:22:38.799301 12672 ag_logging.py:145] Entity <function <lambda> at 0x00000158E3242378> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function <lambda> at 0x00000158E3242378>: ValueError: Failed to parse source code of <function <lambda> at 0x00000158E3242378>, which Python reported as:\n",
      "'function':lambda X: tf.constant(1., dtype='float64', shape=(X.shape[0],1))\n",
      "If this is a lambda function, the error may be avoided by creating the lambda in a standalone statement. Tried to strip down the source to:\n",
      "'function':lambda X: tf.constant(1., dtype='float64', shape=(X.shape[0],1))\n",
      "But that did not work.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <function <lambda> at 0x00000158E3242378> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function <lambda> at 0x00000158E3242378>: ValueError: Failed to parse source code of <function <lambda> at 0x00000158E3242378>, which Python reported as:\n",
      "'function':lambda X: tf.constant(1., dtype='float64', shape=(X.shape[0],1))\n",
      "If this is a lambda function, the error may be avoided by creating the lambda in a standalone statement. Tried to strip down the source to:\n",
      "'function':lambda X: tf.constant(1., dtype='float64', shape=(X.shape[0],1))\n",
      "But that did not work.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1104 22:22:39.472420 12672 ag_logging.py:145] Entity <function <lambda> at 0x00000158E3242378> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function <lambda> at 0x00000158E3242378>: ValueError: Failed to parse source code of <function <lambda> at 0x00000158E3242378>, which Python reported as:\n",
      "'function':lambda X: tf.constant(1., dtype='float64', shape=(X.shape[0],1))\n",
      "If this is a lambda function, the error may be avoided by creating the lambda in a standalone statement. Tried to strip down the source to:\n",
      "'function':lambda X: tf.constant(1., dtype='float64', shape=(X.shape[0],1))\n",
      "But that did not work.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <function <lambda> at 0x00000158E3242378> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function <lambda> at 0x00000158E3242378>: ValueError: Failed to parse source code of <function <lambda> at 0x00000158E3242378>, which Python reported as:\n",
      "'function':lambda X: tf.constant(1., dtype='float64', shape=(X.shape[0],1))\n",
      "If this is a lambda function, the error may be avoided by creating the lambda in a standalone statement. Tried to strip down the source to:\n",
      "'function':lambda X: tf.constant(1., dtype='float64', shape=(X.shape[0],1))\n",
      "But that did not work.\n",
      "Epoch: 1 Loss: 1.751185655593872\n",
      "Epoch: 2 Loss: 1.7869980335235596\n",
      "Epoch: 3 Loss: 1.7339751720428467\n",
      "Epoch: 4 Loss: 1.6635408401489258\n",
      "Epoch: 5 Loss: 1.5914664268493652\n",
      "Epoch: 6 Loss: 1.5219619274139404\n",
      "Epoch: 7 Loss: 1.4561543464660645\n",
      "Epoch: 8 Loss: 1.3941926956176758\n",
      "Epoch: 9 Loss: 1.335890531539917\n",
      "Epoch: 10 Loss: 1.2809468507766724\n",
      "Epoch: 11 Loss: 1.2290282249450684\n",
      "Epoch: 12 Loss: 1.1798033714294434\n",
      "Epoch: 13 Loss: 1.1329598426818848\n",
      "Epoch: 14 Loss: 1.088216781616211\n",
      "Epoch: 15 Loss: 1.045337200164795\n",
      "Epoch: 16 Loss: 1.0041382312774658\n",
      "Epoch: 17 Loss: 0.9644994139671326\n",
      "Epoch: 18 Loss: 0.9263633489608765\n",
      "Epoch: 19 Loss: 0.8897286057472229\n",
      "Epoch: 20 Loss: 0.8546341061592102\n",
      "Epoch: 21 Loss: 0.8211387991905212\n",
      "Epoch: 22 Loss: 0.7892999649047852\n",
      "Epoch: 23 Loss: 0.7591560482978821\n",
      "Epoch: 24 Loss: 0.7307162880897522\n",
      "Epoch: 25 Loss: 0.7039579153060913\n",
      "Epoch: 26 Loss: 0.6788294315338135\n",
      "Epoch: 27 Loss: 0.6552571654319763\n",
      "Epoch: 28 Loss: 0.633152961730957\n",
      "Epoch: 29 Loss: 0.612420916557312\n",
      "Epoch: 30 Loss: 0.592963457107544\n",
      "Epoch: 31 Loss: 0.574684739112854\n",
      "Epoch: 32 Loss: 0.5574935078620911\n",
      "Epoch: 33 Loss: 0.5413042306900024\n",
      "Epoch: 34 Loss: 0.5260379910469055\n",
      "Epoch: 35 Loss: 0.5116222500801086\n",
      "Epoch: 36 Loss: 0.49799075722694397\n",
      "Epoch: 37 Loss: 0.48508337140083313\n",
      "Epoch: 38 Loss: 0.4728453457355499\n",
      "Epoch: 39 Loss: 0.4612269103527069\n",
      "Epoch: 40 Loss: 0.4501829147338867\n",
      "Epoch: 41 Loss: 0.4396722614765167\n",
      "Epoch: 42 Loss: 0.42965757846832275\n",
      "Epoch: 43 Loss: 0.4201046824455261\n",
      "Epoch: 44 Loss: 0.41098257899284363\n",
      "Epoch: 45 Loss: 0.40226277709007263\n",
      "Epoch: 46 Loss: 0.3939192593097687\n",
      "Epoch: 47 Loss: 0.385928213596344\n",
      "Epoch: 48 Loss: 0.37826767563819885\n",
      "Epoch: 49 Loss: 0.37091752886772156\n",
      "Epoch: 50 Loss: 0.36385923624038696\n",
      "Epoch: 51 Loss: 0.35707566142082214\n",
      "Epoch: 52 Loss: 0.35055097937583923\n",
      "Epoch: 53 Loss: 0.34427064657211304\n",
      "Epoch: 54 Loss: 0.33822107315063477\n",
      "Epoch: 55 Loss: 0.3323896825313568\n",
      "Epoch: 56 Loss: 0.326764851808548\n",
      "Epoch: 57 Loss: 0.32133570313453674\n",
      "Epoch: 58 Loss: 0.3160921335220337\n",
      "Epoch: 59 Loss: 0.3110246956348419\n",
      "Epoch: 60 Loss: 0.30612465739250183\n",
      "Epoch: 61 Loss: 0.30138373374938965\n",
      "Epoch: 62 Loss: 0.29679426550865173\n",
      "Epoch: 63 Loss: 0.29234907031059265\n",
      "Epoch: 64 Loss: 0.2880413830280304\n",
      "Epoch: 65 Loss: 0.283864825963974\n",
      "Epoch: 66 Loss: 0.2798135280609131\n",
      "Epoch: 67 Loss: 0.275881826877594\n",
      "Epoch: 68 Loss: 0.2720644474029541\n",
      "Epoch: 69 Loss: 0.26835647225379944\n",
      "Epoch: 70 Loss: 0.26475319266319275\n",
      "Epoch: 71 Loss: 0.26125022768974304\n",
      "Epoch: 72 Loss: 0.25784334540367126\n",
      "Epoch: 73 Loss: 0.254528671503067\n",
      "Epoch: 74 Loss: 0.25130242109298706\n",
      "Epoch: 75 Loss: 0.24816109240055084\n",
      "Epoch: 76 Loss: 0.24510134756565094\n",
      "Epoch: 77 Loss: 0.24211999773979187\n",
      "Epoch: 78 Loss: 0.23921404778957367\n",
      "Epoch: 79 Loss: 0.23638063669204712\n",
      "Epoch: 80 Loss: 0.23361703753471375\n",
      "Epoch: 81 Loss: 0.2309206873178482\n",
      "Epoch: 82 Loss: 0.22828912734985352\n",
      "Epoch: 83 Loss: 0.22572001814842224\n",
      "Epoch: 84 Loss: 0.2232111394405365\n",
      "Epoch: 85 Loss: 0.22076037526130676\n",
      "Epoch: 86 Loss: 0.21836568415164948\n",
      "Epoch: 87 Loss: 0.2160251885652542\n",
      "Epoch: 88 Loss: 0.21373698115348816\n",
      "Epoch: 89 Loss: 0.21149934828281403\n",
      "Epoch: 90 Loss: 0.2093106061220169\n",
      "Epoch: 91 Loss: 0.20716913044452667\n",
      "Epoch: 92 Loss: 0.20507340133190155\n",
      "Epoch: 93 Loss: 0.20302194356918335\n",
      "Epoch: 94 Loss: 0.20101334154605865\n",
      "Epoch: 95 Loss: 0.19904625415802002\n",
      "Epoch: 96 Loss: 0.19711941480636597\n",
      "Epoch: 97 Loss: 0.19523152709007263\n",
      "Epoch: 98 Loss: 0.19338145852088928\n",
      "Epoch: 99 Loss: 0.1915680170059204\n",
      "Epoch: 100 Loss: 0.18979015946388245\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.018562420667654133, 0.02048943429498669)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pmc.measure_accuracy(a=0, b=1, inits=pmc.example1_inits, loss=pmc.example1_loss, exact_function=pmc.psi_e1, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.experimental.list_physical_devices('CPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
