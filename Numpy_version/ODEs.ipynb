{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'divide': 'warn', 'over': 'warn', 'under': 'ignore', 'invalid': 'warn'}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import nnde\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "np.seterr(all='raise')\n",
    "# import pixiedust"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solving ODEs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1\n",
    "\n",
    "$\\frac{d}{dx}\\Psi+(x+\\frac{1+3x^2}{1+x+x^3})\\Psi=x^3+2x+x^2\\frac{1+3x^2}{1+x+x^3}$\n",
    "\n",
    "With boundary initial condition $\\Psi(0)=1$ and domain $x\\in[0,1]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "search = lambda A, B: np.array([i for i in A.flatten().tolist() if i not in B.flatten().tolist()])\n",
    "Xe1_interpolation = np.linspace(0,1,num=1000)\n",
    "Xe1_interpolation = Xe1_interpolation.reshape((Xe1_interpolation.shape[0],1,1))\n",
    "Xe1 = Xe1_interpolation[::100]\n",
    "Xe1_interpolation = search(Xe1_interpolation, Xe1)\n",
    "Xe1_interpolation = Xe1_interpolation.reshape((Xe1_interpolation.shape[0],1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The trial solution for this case is $\\Psi(x)=1 + x N(x)$.\n",
    "The first function below is the function $A(x)=1$\n",
    "and the second function is the function $B(x)=x$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def example1_initial_value(point):\n",
    "  return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def example1_boundary_vanishing(point):\n",
    "  return point"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the loss function for a single point and a whole set\n",
    "\n",
    "The loss function is based on the formula:\n",
    "$$Loss(N)=\\sum_i \\left(L\\Psi(x_i, N(x_i))-f(x_i,\\Psi(x_i, N(x_i))) \\right)^2$$\n",
    "Where N(x) is the neural network and L is some differential operator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def example1_loss_function_single_point(self, point, non_squared=False, *kwargs):\n",
    "  N = self.forward_pass(point, 0)\n",
    "  dN = self.forward_pass(point, 1)\n",
    "  loss = (\n",
    "      point * dN + N + (point + (1 + 3 * point ** 2)/(1 + point + point ** 3)) * (1 + point * N) \n",
    "      - point ** 3 - 2 * point - point ** 2 *(1 + 3 * point ** 2)/(1 + point + point ** 3)\n",
    "    )\n",
    "  if not non_squared:\n",
    "    loss = loss ** 2\n",
    "  return loss[0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def example1_loss_function(self, samples, *kwargs):\n",
    "  loss = 0\n",
    "  for i in range(samples.shape[0]):\n",
    "    loss += self.loss_function_single_point(self, samples[i])\n",
    "  return loss/samples.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the update rules\n",
    "\n",
    "The following functions represent $\\frac{\\partial Loss}{\\partial \\vec{b}}$, $\\frac{\\partial Loss}{\\partial H}$, and $\\frac{\\partial Loss}{\\partial V}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def example1_bias_change(self, point, label, *kwargs):\n",
    "  db = np.zeros((self.hidden_dim, 1)).astype(dtype=\"float64\")\n",
    "  loss_sqrt = self.loss_function_single_point(self, point, non_squared=True)\n",
    "  db_N = self.network_derivative_bias(point, 0)\n",
    "  db_DN = self.network_derivative_bias(point, 1)\n",
    "  point = point.reshape((1,))\n",
    "  for m in range(self.hidden_dim):\n",
    "    db[m] += 2 * loss_sqrt * (\n",
    "      point * db_DN[0, 0, m] + db_N[0, 0, m] + (point + (1 + 3 * point ** 2)/(1 + point + point ** 3)) * point * db_N[0, 0, m])\n",
    "  return db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def example1_hidden_weights_change(self, point, *kwargs):\n",
    "  dH = np.zeros((self.hidden_dim, self.input_dim)).astype(dtype=\"float64\")\n",
    "  loss_sqrt = self.loss_function_single_point(self, point, non_squared=True)\n",
    "  dH_N = self.network_derivative_hidden_weights(point, 0)\n",
    "  dH_DN = self.network_derivative_hidden_weights(point, 1)\n",
    "  for m in range(self.hidden_dim):\n",
    "    for p in range(self.input_dim):\n",
    "      dH[m, p] += 2 * loss_sqrt * (\n",
    "        point * dH_DN[0, 0, m, p] + dH_N[0, 0, m, p] + (point + (1 + 3 * point ** 2)/(1 + point + point ** 3)) * point * dH_N[0, 0, m, p])\n",
    "  return dH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def example1_visible_weights_change(self, point, *kwargs):\n",
    "  dV = np.zeros((self.visible_dim, self.hidden_dim)).astype(dtype=\"float64\")\n",
    "  loss_sqrt = self.loss_function_single_point(self, point, non_squared=True)\n",
    "  dV_N = self.network_derivative_visible_weights(point, 0)\n",
    "  dV_DN = self.network_derivative_visible_weights(point, 1)\n",
    "  for m in range(self.visible_dim):\n",
    "    for p in range(self.hidden_dim):\n",
    "      dV[m, p] += 2 * loss_sqrt * (\n",
    "        point * dV_DN[0, 0, m, p] + dV_N[0, 0, m, p] + (point + (1 + 3 * point ** 2)/(1 + point + point ** 3)) * point * dV_N[0, 0, m, p])\n",
    "  return dV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the trial solution with an apropiate network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "example1_trial_solution = nnde.TrialSolution(loss_function=example1_loss_function_single_point,\n",
    "                                        boundary_condition_value_function=example1_initial_value,\n",
    "                                        boundary_vanishing_function=example1_boundary_vanishing,\n",
    "                                        input_dim=1, hidden_dim=10, output_dim=1, learning_rate=1e-1, momentum=1e-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "train() missing 1 required positional argument: 'Y'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-594896f56f97>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mexample1_trial_solution\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mXe1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1e-1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Documents\\Repos\\NNDE\\Numpy_version\\trialsolution.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, samples, epochs, batch_size, learning_rate)\u001b[0m\n\u001b[0;32m     84\u001b[0m         '''\n\u001b[0;32m     85\u001b[0m         self.network.fit(X=samples, epochs=epochs, batch_size=batch_size,\n\u001b[1;32m---> 86\u001b[1;33m         learning_rate=learning_rate)\n\u001b[0m",
      "\u001b[1;32m~\\Documents\\Repos\\NNDE\\Numpy_version\\shallownetwork.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, epochs, batch_size, learning_rate, verbose, message_frequency, **kwargs)\u001b[0m\n\u001b[0;32m    118\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[0mbatched_counter\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    119\u001b[0m                         self.optimizer.train(network=self, X=X[batched_counter:batched_counter+batch_size, :],\n\u001b[1;32m--> 120\u001b[1;33m                                              learning_rate=learning_rate, **kwargs)\n\u001b[0m\u001b[0;32m    121\u001b[0m                     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    122\u001b[0m                         self.optimizer.train(network=self, X=X[batched_counter:, :],\n",
      "\u001b[1;31mTypeError\u001b[0m: train() missing 1 required positional argument: 'Y'"
     ]
    }
   ],
   "source": [
    "example1_trial_solution.train(samples=Xe1, epochs=100, batch_size=-1, learning_rate=1e-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the results \n",
    "\n",
    "The numerical solution (training set - red, valdiaiton set - green) along with the analytical solution (blue)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xe1plot = np.arange(0,1.5, 0.01)\n",
    "Xe1plot = Xe1plot.reshape((Xe1plot.shape[0], 1, 1))\n",
    "Ye1 = np.array([example1_trial_solution.predict(Xe1plot[i]) for i in range(Xe1plot.shape[0])]).reshape((Xe1plot.shape[0],))\n",
    "Xe1plot = Xe1plot.reshape((Xe1plot.shape[0],))\n",
    "plt.scatter(Xe1plot, Ye1, c='g', label='Numerical - validation', marker='x', s=1)\n",
    "psi_e1 = lambda x:  np.exp(-0.5*x**2)/(1+x+x**3) + x**2\n",
    "Ye12 = np.array([psi_e1(Xe1plot[i]) for i in range(Xe1plot.shape[0])])\n",
    "plt.plot(Xe1plot, Ye12, c='b', label='Analytic')\n",
    "Ye1sol = np.array([example1_trial_solution.predict(Xe1[i]) for i in range(Xe1.shape[0])]).reshape((Xe1.shape[0],))\n",
    "plt.scatter(Xe1.reshape((Xe1.shape[0],)), Ye1sol, c='r', label='Numerical - Training', marker='+', s=30)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ye1_interpolation_predict = np.array([example1_trial_solution.predict(Xe1_interpolation[i]) for i in range(Xe1_interpolation.shape[0])]).reshape((Xe1_interpolation.shape[0],))\n",
    "Ye1_interpolation_true = np.array([psi_e1(Xe1_interpolation[i]) for i in range(Xe1_interpolation.shape[0])]).reshape((Xe1_interpolation.shape[0],))\n",
    "np.abs(Ye1_interpolation_true - Ye1_interpolation_predict).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.abs(Ye1sol - np.array([psi_e1(Xe1[i]) for i in range(Xe1.shape[0])]).reshape((Xe1.shape[0],))).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.clf()\n",
    "plt.scatter(Xe1plot, Ye1, c='xkcd:sky blue', label='Numerical - validation', marker='x', s=300)\n",
    "plt.scatter(Xe1.reshape((Xe1.shape[0],)), Ye1sol, c='r', label='Numerical - Training', marker='+', s=1000)\n",
    "plt.plot(Xe1plot, Ye12, c='xkcd:goldenrod', label='Analytic', linewidth=5)\n",
    "plt.xlabel(r'$x$', fontsize='50')\n",
    "plt.ylabel(r'$y$', fontsize='50')\n",
    "plt.xlim((0,1.5))\n",
    "plt.ylim((0.7,2.2))\n",
    "# plt.axis('equal')\n",
    "plt.legend(fontsize='40')\n",
    "plt.title('Example 1', fontsize='60')\n",
    "plt.gcf().set_size_inches(30, 22.5)\n",
    "plt.tick_params(axis='both', which='major', labelsize=35)\n",
    "plt.savefig('plots/example1.jpg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2\n",
    "\n",
    "$\\frac{d}{dx}\\Psi+\\frac{1}{5}\\Psi=\\exp(-\\frac{x}{5})\\cos(x)$\n",
    "\n",
    "With boundary initial condition $\\Psi(0)=0$ and domain $x\\in[0,2]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xe2_interpolation = np.linspace(0,2,num=1000)\n",
    "Xe2_interpolation = Xe2_interpolation.reshape((Xe2_interpolation.shape[0],1,1))\n",
    "Xe2 = Xe2_interpolation[::100]\n",
    "Xe2_interpolation = search(Xe2_interpolation, Xe2)\n",
    "Xe2_interpolation = Xe2_interpolation.reshape((Xe2_interpolation.shape[0],1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The trial solution for this case is $\\Psi(x)=x N(x)$.\n",
    "The first function below is the function $A(x)=0$\n",
    "and the second function is the function $B(x)=x$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def example2_initial_value(point):\n",
    "  return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def example2_boundary_vanishing(point):\n",
    "  return point"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the loss function for a single point and a whole set\n",
    "\n",
    "The loss function is based on the formula:\n",
    "$$Loss(N)=\\sum_i \\left(L\\Psi(x_i, N(x_i))-f(x_i,\\Psi(x_i, N(x_i))) \\right)^2$$\n",
    "Where N(x) is the neural network and L is some differential operator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def example2_loss_function_single_point(self, point, non_squared=False, *kwargs):\n",
    "  N = self.forward_pass(point, 0)\n",
    "  dN = self.forward_pass(point, 1)\n",
    "  loss = (\n",
    "      point * dN + N + 0.2 * point * N - np.exp(-0.2*point)*np.cos(point)\n",
    "    )\n",
    "  if not non_squared:\n",
    "    loss = loss ** 2\n",
    "  return loss[0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def example2_loss_function(self, samples, *kwargs):\n",
    "  loss = 0\n",
    "  for i in range(samples.shape[0]):\n",
    "    loss += self.loss_function_single_point(self, samples[i])\n",
    "  return loss/samples.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the update rules\n",
    "\n",
    "The following functions represent $\\frac{\\partial Loss}{\\partial \\vec{b}}$, $\\frac{\\partial Loss}{\\partial H}$, and $\\frac{\\partial Loss}{\\partial V}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def example2_bias_change(self, point, label, *kwargs):\n",
    "  db = np.zeros((self.hidden_dim, 1)).astype(dtype=\"float64\")\n",
    "  loss_sqrt = self.loss_function_single_point(self, point, non_squared=True)\n",
    "  db_N = self.network_derivative_bias(point, 0)\n",
    "  db_DN = self.network_derivative_bias(point, 1)\n",
    "  point = point.reshape((1,))\n",
    "  for m in range(self.hidden_dim):\n",
    "    db[m] += 2 * loss_sqrt * ( point * db_DN[0, 0, m] + db_N[0, 0, m] + 0.2 * point * db_N[0, 0, m])\n",
    "  return db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def example2_hidden_weights_change(self, point, *kwargs):\n",
    "  dH = np.zeros((self.hidden_dim, self.input_dim)).astype(dtype=\"float64\")\n",
    "  loss_sqrt = self.loss_function_single_point(self, point, non_squared=True)\n",
    "  dH_N = self.network_derivative_hidden_weights(point, 0)\n",
    "  dH_DN = self.network_derivative_hidden_weights(point, 1)\n",
    "  for m in range(self.hidden_dim):\n",
    "    for p in range(self.input_dim):\n",
    "      dH[m, p] += 2 * loss_sqrt * ( point * dH_DN[0, 0, m, p] + dH_N[0, 0, m, p] + 0.2 * point * dH_N[0, 0, m, p])\n",
    "  return dH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def example2_visible_weights_change(self, point, *kwargs):\n",
    "  dV = np.zeros((self.visible_dim, self.hidden_dim)).astype(dtype=\"float64\")\n",
    "  loss_sqrt = self.loss_function_single_point(self, point, non_squared=True)\n",
    "  dV_N = self.network_derivative_visible_weights(point, 0)\n",
    "  dV_DN = self.network_derivative_visible_weights(point, 1)\n",
    "  for m in range(self.visible_dim):\n",
    "    for p in range(self.hidden_dim):\n",
    "      dV[m, p] += 2 * loss_sqrt * (point * dV_DN[0, 0, m, p] + dV_N[0, 0, m, p] + 0.2 * point * dV_N[0, 0, m, p])\n",
    "  return dV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the trial solution with an apropiate network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example2_trial_solution = nnde.TrialSolution(loss_function=example2_loss_function,\n",
    "                                        loss_function_single_point=example2_loss_function_single_point,\n",
    "                                        bias_change=example2_bias_change,\n",
    "                                        hidden_weights_change=example2_hidden_weights_change,\n",
    "                                        visible_weights_change=example2_visible_weights_change,\n",
    "                                        boundary_condition_value_function=example2_initial_value,\n",
    "                                        boundary_vanishing_function=example2_boundary_vanishing,\n",
    "                                        input_dim=1, hidden_dim=10, output_dim=1, learning_rate=1e-1, momentum=1e-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "example2_trial_solution.train(Xe2, 10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the results \n",
    "\n",
    "The numerical solution (training set - red, valdiaiton set - green) along with the analytical solution (blue)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ye2sol = np.array([example2_trial_solution.predict(Xe2[i]) for i in range(Xe2.shape[0])]).reshape((Xe2.shape[0],))\n",
    "plt.scatter(Xe2.reshape((Xe2.shape[0],)), Ye2sol, c='r', label='Numerical - Training', marker='+', s=30)\n",
    "psi_e2 = lambda x: np.exp(-0.2*x) * np.sin(x)\n",
    "Xe2plot = np.arange(0,2.5, 0.01)\n",
    "Xe2plot = Xe2plot.reshape((Xe2plot.shape[0], 1, 1))\n",
    "Ye2 = np.array([example2_trial_solution.predict(Xe2plot[i]) for i in range(Xe2plot.shape[0])]).reshape((Xe2plot.shape[0],))\n",
    "Xe2plot = Xe2plot.reshape((Xe2plot.shape[0],))\n",
    "Ye22 = np.array([np.exp(-0.2*Xe2plot[i]) * np.sin(Xe2plot[i]) for i in range(Xe2plot.shape[0])])\n",
    "plt.scatter(Xe2plot, Ye2, c='g', label='Numerical- Validation', marker='x', s=1)\n",
    "plt.plot(Xe2plot, Ye22, c='b', label='Analytic')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Ye2_interpolation_predict = np.array([example2_trial_solution.predict(Xe2_interpolation[i]) for i in range(Xe2_interpolation.shape[0])]).reshape((Xe2_interpolation.shape[0],))\n",
    "Ye2_interpolation_true = np.array([psi_e2(Xe2_interpolation[i]) for i in range(Xe2_interpolation.shape[0])]).reshape((Xe2_interpolation.shape[0],))\n",
    "np.abs(Ye2_interpolation_true - Ye2_interpolation_predict).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.abs(Ye2sol - np.array([psi_e2(Xe2[i]) for i in range(Xe2.shape[0])]).reshape((Xe2.shape[0],))).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.clf()\n",
    "plt.scatter(Xe2plot, Ye2, c='xkcd:sky blue', label='Numerical - validation', marker='x', s=300)\n",
    "plt.scatter(Xe2.reshape((Xe2.shape[0],)), Ye2sol, c='r', label='Numerical - Training', marker='+', s=1000)\n",
    "plt.plot(Xe2plot, Ye22, c='xkcd:goldenrod', label='Analytic', linewidth=5)\n",
    "plt.xlabel(r'$x$', fontsize='50')\n",
    "plt.ylabel(r'$y$', fontsize='50')\n",
    "plt.xlim((0,2.5))\n",
    "plt.ylim((0.,1))\n",
    "# plt.axis('equal')\n",
    "plt.legend(fontsize='40')\n",
    "plt.title('Example 2', fontsize='60')\n",
    "plt.gcf().set_size_inches(30, 22.5)\n",
    "plt.tick_params(axis='both', which='major', labelsize=35)\n",
    "plt.savefig('plots/example2.jpg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 3\n",
    "\n",
    "$\\frac{d^2}{dx^2}\\Psi+\\frac{1}{5}\\frac{d}{dx}\\Psi+\\Psi=-\\frac{1}{5}\\exp(-\\frac{x}{5})\\cos(x)$\n",
    "\n",
    "With boundary initial condition $\\Psi(0)=0$, $\\frac{d}{dx}\\Psi(0)=1$ and domain $x\\in[0,2]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpolation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xe3_interpolation = np.linspace(0,2,num=1000)\n",
    "Xe3_interpolation = Xe3_interpolation.reshape((Xe3_interpolation.shape[0],1,1))\n",
    "Xe3 = Xe3_interpolation[::100]\n",
    "Xe3_interpolation = search(Xe3_interpolation, Xe3)\n",
    "Xe3_interpolation = Xe3_interpolation.reshape((Xe3_interpolation.shape[0],1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The trial solution for this case is $\\Psi(x)=x + x^2N(x)$.\n",
    "The first function below is the function $A(x)=x$\n",
    "and the second function is the function $B(x)=x^2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def example3_initial_value(point):\n",
    "  return point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def example3_boundary_vanishing(point):\n",
    "  return point ** 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the loss function for a single point and a whole set\n",
    "\n",
    "The loss function is based on the formula:\n",
    "$$Loss(N)=\\sum_i \\left(L\\Psi(x_i, N(x_i))-f(x_i,\\Psi(x_i, N(x_i))) \\right)^2$$\n",
    "Where N(x) is the neural network and L is some differential operator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def example3_loss_function_single_point(self, point, non_squared=False, *kwargs):\n",
    "  N = self.forward_pass(point, 0)\n",
    "  dN = self.forward_pass(point, 1)\n",
    "  d2N = self.forward_pass(point, 2)\n",
    "  loss = ( 2 * N + 4 * point * dN + point ** 2 * d2N + 0.2 * (1 + 2 * point * N + point ** 2 * dN)\n",
    "     + point + point ** 2 * N + 0.2 * np.exp(-0.2*point)*np.cos(point)\n",
    "    )\n",
    "  if not non_squared:\n",
    "    loss = loss ** 2\n",
    "  return loss[0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def example3_loss_function(self, samples, *kwargs):\n",
    "  loss = 0\n",
    "  for i in range(samples.shape[0]):\n",
    "    loss += self.loss_function_single_point(self, samples[i])\n",
    "  return loss/samples.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the update rules\n",
    "\n",
    "The following functions represent $\\frac{\\partial Loss}{\\partial \\vec{b}}$, $\\frac{\\partial Loss}{\\partial H}$, and $\\frac{\\partial Loss}{\\partial V}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def example3_bias_change(self, point, label, *kwargs):\n",
    "  db = np.zeros((self.hidden_dim, 1)).astype(dtype=\"float64\")\n",
    "  loss_sqrt = self.loss_function_single_point(self, point, non_squared=True)\n",
    "  db_N = self.network_derivative_bias(point, 0)\n",
    "  db_DN = self.network_derivative_bias(point, 1)\n",
    "  db_D2N = self.network_derivative_bias(point, 2)\n",
    "  point = point.reshape((1,))\n",
    "  for m in range(self.hidden_dim):\n",
    "    db[m] += 2 * loss_sqrt * ( 2 * db_N[0, 0, m] + 4 * point * db_DN[0, 0, m] + point ** 2 * db_D2N[0, 0, m]\n",
    "                              + 0.2 * (2 * point * db_N[0, 0, m] + point ** 2 * db_DN[0, 0, m])\n",
    "                              + point ** 2 * db_N[0, 0, m] \n",
    "      )\n",
    "  return db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def example3_hidden_weights_change(self, point, *kwargs):\n",
    "  dH = np.zeros((self.hidden_dim, self.input_dim)).astype(dtype=\"float64\")\n",
    "  loss_sqrt = self.loss_function_single_point(self, point, non_squared=True)\n",
    "  dH_N = self.network_derivative_hidden_weights(point, 0)\n",
    "  dH_DN = self.network_derivative_hidden_weights(point, 1)\n",
    "  dH_D2N = self.network_derivative_hidden_weights(point, 2)\n",
    "  for m in range(self.hidden_dim):\n",
    "    for p in range(self.input_dim):\n",
    "      dH[m, p] += 2 * loss_sqrt * (2 * dH_N[0, 0, m, p] + 4 * point * dH_DN[0, 0, m, p] + point ** 2 * dH_D2N[0, 0, m, p]\n",
    "                              + 0.2 * (2 * point * dH_N[0, 0, m, p] + point ** 2 * dH_DN[0, 0, m, p])\n",
    "                              + point ** 2 * dH_N[0, 0, m, p]\n",
    "      )\n",
    "  return dH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def example3_visible_weights_change(self, point, *kwargs):\n",
    "  dV = np.zeros((self.visible_dim, self.hidden_dim)).astype(dtype=\"float64\")\n",
    "  loss_sqrt = self.loss_function_single_point(self, point, non_squared=True)\n",
    "  dV_N = self.network_derivative_visible_weights(point, 0)\n",
    "  dV_DN = self.network_derivative_visible_weights(point, 1)\n",
    "  dV_D2N = self.network_derivative_visible_weights(point, 2)\n",
    "  for m in range(self.visible_dim):\n",
    "    for p in range(self.hidden_dim):\n",
    "      dV[m, p] += 2 * loss_sqrt * (2 * dV_N[0, 0, m, p] + 4 * point * dV_DN[0, 0, m, p] + point ** 2 * dV_D2N[0, 0, m, p]\n",
    "                              + 0.2 * (2 * point * dV_N[0, 0, m, p] + point ** 2 * dV_DN[0, 0, m, p])\n",
    "                              + point ** 2 * dV_N[0, 0, m, p]   \n",
    "      )\n",
    "  return dV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the trial solution with an apropiate network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example3_trial_solution = nnde.TrialSolution(loss_function=example3_loss_function,\n",
    "                                        loss_function_single_point=example3_loss_function_single_point,\n",
    "                                        bias_change=example3_bias_change,\n",
    "                                        hidden_weights_change=example3_hidden_weights_change,\n",
    "                                        visible_weights_change=example3_visible_weights_change,\n",
    "                                        boundary_condition_value_function=example3_initial_value,\n",
    "                                        boundary_vanishing_function=example3_boundary_vanishing,\n",
    "                                        input_dim=1, hidden_dim=10, output_dim=1, learning_rate=1e-2, momentum=1e-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "example3_trial_solution.train(Xe3, 10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the results \n",
    "\n",
    "The numerical solution (training set - red, valdiaiton set - green) along with the analytical solution (blue)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Ye3sol = np.array([example3_trial_solution.predict(Xe3[i]) for i in range(Xe3.shape[0])]).reshape((Xe3.shape[0],))\n",
    "plt.scatter(Xe3.reshape((Xe3.shape[0],)), Ye3sol, c='r', label='Numerical - Training', marker='+', s=30)\n",
    "Xe3plot = np.arange(0,2.5, 0.01)\n",
    "Xe3plot = Xe3plot.reshape((Xe3plot.shape[0], 1, 1))\n",
    "Ye3 = np.array([example3_trial_solution.predict(Xe3plot[i]) for i in range(Xe3plot.shape[0])]).reshape((Xe3plot.shape[0],))\n",
    "Xe3plot = Xe3plot.reshape((Xe3plot.shape[0],))\n",
    "psi_e3 = lambda x: np.exp(-0.2*x) * np.sin(x)\n",
    "Ye33 = np.array([psi_e3(Xe3plot[i]) for i in range(Xe3plot.shape[0])])\n",
    "plt.scatter(Xe3plot, Ye3, c='g', label='Numerical- Validation', marker='x', s=1)\n",
    "plt.plot(Xe3plot, Ye33, c='b', label='Analytic')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(example3_trial_solution.network.hidden_layer.bias.min())\n",
    "print(example3_trial_solution.network.hidden_layer.bias.max())\n",
    "print(example3_trial_solution.network.hidden_layer.weights.min())\n",
    "print(example3_trial_solution.network.hidden_layer.weights.max())\n",
    "print(example3_trial_solution.network.visible_layer.weights.min())\n",
    "print(example3_trial_solution.network.visible_layer.weights.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Ye3_interpolation_predict = np.array([example3_trial_solution.predict(Xe3_interpolation[i]) for i in range(Xe3_interpolation.shape[0])]).reshape((Xe3_interpolation.shape[0],))\n",
    "Ye3_interpolation_true = np.array([psi_e3(Xe3_interpolation[i]) for i in range(Xe3_interpolation.shape[0])]).reshape((Xe3_interpolation.shape[0],))\n",
    "np.abs(Ye3_interpolation_true - Ye3_interpolation_predict).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.abs(Ye3sol - np.array([psi_e3(Xe3[i]) for i in range(Xe3.shape[0])]).reshape((Xe3.shape[0],))).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.clf()\n",
    "plt.scatter(Xe3plot, Ye3, c='xkcd:sky blue', label='Numerical - validation', marker='x', s=300)\n",
    "plt.scatter(Xe3.reshape((Xe3.shape[0],)), Ye3sol, c='r', label='Numerical - Training', marker='+', s=1000)\n",
    "plt.plot(Xe3plot, Ye33, c='xkcd:goldenrod', label='Analytic', linewidth=5)\n",
    "plt.xlabel(r'$x$', fontsize='50')\n",
    "plt.ylabel(r'$y$', fontsize='50')\n",
    "plt.xlim((0,2.5))\n",
    "plt.ylim((0.,0.8))\n",
    "# plt.axis('equal')\n",
    "plt.legend(fontsize='40')\n",
    "plt.title('Example 3', fontsize='60')\n",
    "plt.gcf().set_size_inches(30, 22.5)\n",
    "plt.tick_params(axis='both', which='major', labelsize=35)\n",
    "plt.savefig('plots/example3.jpg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
